# Task03 å­¦ä¹ ç¬”è®° - ç¬¬å…«ç« ï¼šè®°å¿†ä¸æ£€ç´¢

**å­¦ä¹ æ—¥æœŸ**: 2025-12-22  
**ç« èŠ‚**: ç¬¬å…«ç«  - Agentçš„è®°å¿†ç³»ç»Ÿ

---

## ğŸ“– 8.1 ä¸ºä»€ä¹ˆAgentéœ€è¦è®°å¿†ï¼Ÿ

### ä¼ ç»ŸLLMçš„å±€é™æ€§

#### 1. **ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶**
- GPT-3.5: ~4K tokens
- GPT-4: 8K-32K tokens  
- Claude: 100K tokens
- **é—®é¢˜**: é•¿å¯¹è¯ä¼šè¶…å‡ºä¸Šä¸‹æ–‡çª—å£ï¼Œä¸¢å¤±å†å²ä¿¡æ¯

#### 2. **æ— çŠ¶æ€æ€§**
```python
# ä¼ ç»ŸLLMæ¯æ¬¡è°ƒç”¨éƒ½æ˜¯ç‹¬ç«‹çš„
response1 = llm("æˆ‘å«å¼ ä¸‰")  # LLMå›å¤ï¼šä½ å¥½å¼ ä¸‰
response2 = llm("æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ")  # LLMå›å¤ï¼šæˆ‘ä¸çŸ¥é“ä½ çš„åå­—
# âŒ LLMæ— æ³•è®°ä½ä¸Šä¸€è½®å¯¹è¯
```

#### 3. **çŸ¥è¯†æˆªæ­¢æ—¥æœŸ**
- è®­ç»ƒæ•°æ®æœ‰æ—¶é—´é™åˆ¶
- æ— æ³•è·å–æœ€æ–°ä¿¡æ¯
- æ— æ³•å­¦ä¹ ç”¨æˆ·ç‰¹å®šçŸ¥è¯†

### Agentè®°å¿†ç³»ç»Ÿçš„ä»·å€¼

#### âœ… **é•¿æœŸå¯¹è¯èƒ½åŠ›**
- ç»´æŠ¤å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡
- è®°ä½ç”¨æˆ·åå¥½å’Œå†å²
- å®ç°è¿ç»­æ€§äº¤äº’

#### âœ… **ä¸ªæ€§åŒ–äº¤äº’**
- è®°ä½ç”¨æˆ·ä¿¡æ¯ï¼ˆå§“åã€åå¥½ç­‰ï¼‰
- é€‚åº”ç”¨æˆ·ä¹ æƒ¯
- æä¾›å®šåˆ¶åŒ–æœåŠ¡

#### âœ… **çŸ¥è¯†ç§¯ç´¯**
- å­˜å‚¨é¢†åŸŸçŸ¥è¯†
- å­¦ä¹ æ–°ä¿¡æ¯
- æ„å»ºçŸ¥è¯†åº“

---

## ğŸ“ 8.2 çŸ­æœŸè®°å¿† (Short-term Memory)

### æ¦‚å¿µå®šä¹‰
**çŸ­æœŸè®°å¿†**: ç”¨äºç»´æŠ¤å½“å‰å¯¹è¯ä¼šè¯çš„ä¸´æ—¶ä¸Šä¸‹æ–‡ä¿¡æ¯

### æ ¸å¿ƒåŠŸèƒ½
1. **å¯¹è¯å†å²ç®¡ç†**: å­˜å‚¨æœ€è¿‘çš„å¯¹è¯æ¶ˆæ¯
2. **æ»‘åŠ¨çª—å£ç­–ç•¥**: ä¿æŒå›ºå®šæ•°é‡çš„æœ€è¿‘æ¶ˆæ¯
3. **Tokenä¼˜åŒ–**: ç®¡ç†ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œé¿å…è¶…é™
4. **ä¸Šä¸‹æ–‡å‹ç¼©**: æ€»ç»“æˆ–åˆ é™¤ä¸é‡è¦çš„ä¿¡æ¯

### å®ç°ç­–ç•¥

#### ç­–ç•¥1: å›ºå®šçª—å£å¤§å°
```python
class ShortTermMemory:
    def __init__(self, max_messages=10):
        self.messages = []
        self.max_messages = max_messages
    
    def add_message(self, role, content):
        self.messages.append({"role": role, "content": content})
        # ä¿æŒçª—å£å¤§å°
        if len(self.messages) > self.max_messages:
            self.messages.pop(0)  # åˆ é™¤æœ€æ—©çš„æ¶ˆæ¯
```

**ä¼˜ç‚¹**: ç®€å•æ˜“å®ç°  
**ç¼ºç‚¹**: å¯èƒ½åˆ é™¤é‡è¦ä¿¡æ¯

#### ç­–ç•¥2: Tokenæ•°é‡é™åˆ¶
```python
class TokenLimitedMemory:
    def __init__(self, max_tokens=4000):
        self.messages = []
        self.max_tokens = max_tokens
    
    def add_message(self, role, content):
        self.messages.append({"role": role, "content": content})
        # åˆ é™¤æ¶ˆæ¯ç›´åˆ°tokenæ•°é‡æ»¡è¶³è¦æ±‚
        while self.count_tokens() > self.max_tokens:
            self.messages.pop(0)
    
    def count_tokens(self):
        # è®¡ç®—æ‰€æœ‰æ¶ˆæ¯çš„tokenæ€»æ•°
        return sum(len(msg["content"]) // 4 for msg in self.messages)
```

**ä¼˜ç‚¹**: æ›´ç²¾ç¡®åœ°æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦  
**ç¼ºç‚¹**: éœ€è¦tokenè®¡æ•°å·¥å…·

#### ç­–ç•¥3: é‡è¦æ€§åŠ æƒ
```python
class ImportanceBasedMemory:
    def add_message(self, role, content, importance=1.0):
        self.messages.append({
            "role": role,
            "content": content,
            "importance": importance
        })
    
    def trim_memory(self):
        # åˆ é™¤é‡è¦æ€§ä½çš„æ¶ˆæ¯
        self.messages.sort(key=lambda x: x["importance"], reverse=True)
        self.messages = self.messages[:self.max_messages]
```

**ä¼˜ç‚¹**: ä¿ç•™é‡è¦ä¿¡æ¯  
**ç¼ºç‚¹**: éœ€è¦è¯„ä¼°æ¶ˆæ¯é‡è¦æ€§

### ä½¿ç”¨åœºæ™¯
- âœ… å¤šè½®å¯¹è¯
- âœ… èŠå¤©æœºå™¨äºº
- âœ… å®¢æœç³»ç»Ÿ
- âœ… å®æ—¶äº¤äº’

---

## ğŸ—„ï¸ 8.3 é•¿æœŸè®°å¿† (Long-term Memory)

### æ¦‚å¿µå®šä¹‰
**é•¿æœŸè®°å¿†**: æŒä¹…åŒ–å­˜å‚¨ï¼Œç”¨äºä¿å­˜é•¿æœŸçŸ¥è¯†å’Œå†å²ä¿¡æ¯

### æ ¸å¿ƒæŠ€æœ¯

#### 1. **å‘é‡æ•°æ®åº“ (Vector Database)**

ä¸»æµå‘é‡æ•°æ®åº“å¯¹æ¯”ï¼š

| æ•°æ®åº“ | ç±»å‹ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|--------|------|------|----------|
| **ChromaDB** | åµŒå…¥å¼ | è½»é‡ã€æ˜“ç”¨ã€å…è´¹ | å¼€å‘æµ‹è¯•ã€å°è§„æ¨¡åº”ç”¨ |
| **Pinecone** | äº‘æœåŠ¡ | é«˜æ€§èƒ½ã€æ‰˜ç®¡ | ç”Ÿäº§ç¯å¢ƒã€å¤§è§„æ¨¡åº”ç”¨ |
| **Weaviate** | è‡ªæ‰˜ç®¡ | åŠŸèƒ½ä¸°å¯Œã€å¼€æº | ä¼ä¸šçº§åº”ç”¨ |
| **Milvus** | åˆ†å¸ƒå¼ | é«˜å¹¶å‘ã€å¯æ‰©å±• | æµ·é‡æ•°æ® |

#### 2. **Embedding (æ–‡æœ¬å‘é‡åŒ–)**

```python
from sentence_transformers import SentenceTransformer

# åŠ è½½Embeddingæ¨¡å‹
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# æ–‡æœ¬è½¬å‘é‡
text = "Agentéœ€è¦è®°å¿†ç³»ç»Ÿæ¥ç»´æŠ¤é•¿æœŸå¯¹è¯èƒ½åŠ›"
embedding = model.encode(text)  # è¿”å› 384ç»´å‘é‡

print(f"å‘é‡ç»´åº¦: {len(embedding)}")  # 384
```

**å¸¸ç”¨Embeddingæ¨¡å‹**:
- `all-MiniLM-L6-v2`: è‹±æ–‡ï¼Œè½»é‡çº§ï¼ˆ384ç»´ï¼‰
- `paraphrase-multilingual-MiniLM-L12-v2`: å¤šè¯­è¨€ï¼ˆ384ç»´ï¼‰
- `text-embedding-ada-002`: OpenAIï¼Œæ•ˆæœå¥½ä½†æ”¶è´¹ï¼ˆ1536ç»´ï¼‰
- `bge-large-zh`: ä¸­æ–‡ï¼Œæ•ˆæœä¼˜ç§€ï¼ˆ1024ç»´ï¼‰

#### 3. **è¯­ä¹‰æ£€ç´¢ (Semantic Search)**

```python
import chromadb

# åˆ›å»ºå‘é‡æ•°æ®åº“
client = chromadb.Client()
collection = client.create_collection("agent_memory")

# å­˜å‚¨æ–‡æ¡£
collection.add(
    documents=["Agentéœ€è¦è®°å¿†", "RAGæ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆ"],
    ids=["doc1", "doc2"]
)

# è¯­ä¹‰æ£€ç´¢
results = collection.query(
    query_texts=["ä»€ä¹ˆæ˜¯è®°å¿†ç³»ç»Ÿï¼Ÿ"],
    n_results=2
)
```

**å·¥ä½œåŸç†**:
1. å°†æŸ¥è¯¢æ–‡æœ¬è½¬ä¸ºå‘é‡
2. è®¡ç®—æŸ¥è¯¢å‘é‡ä¸æ•°æ®åº“ä¸­æ‰€æœ‰å‘é‡çš„ç›¸ä¼¼åº¦
3. è¿”å›æœ€ç›¸ä¼¼çš„Top-Kç»“æœ

#### 4. **ç›¸ä¼¼åº¦è®¡ç®—**

å¸¸ç”¨ç›¸ä¼¼åº¦åº¦é‡ï¼š

**ä½™å¼¦ç›¸ä¼¼åº¦ (Cosine Similarity)**:
```python
import numpy as np

def cosine_similarity(vec1, vec2):
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

# å€¼èŒƒå›´: [-1, 1]ï¼Œè¶Šæ¥è¿‘1è¶Šç›¸ä¼¼
```

**æ¬§æ°è·ç¦» (Euclidean Distance)**:
```python
def euclidean_distance(vec1, vec2):
    return np.linalg.norm(vec1 - vec2)

# å€¼èŒƒå›´: [0, âˆ]ï¼Œè¶Šå°è¶Šç›¸ä¼¼
```

### ä½¿ç”¨åœºæ™¯
- âœ… çŸ¥è¯†åº“é—®ç­”
- âœ… æ–‡æ¡£æ£€ç´¢
- âœ… ä¸ªæ€§åŒ–æ¨è
- âœ… å†å²è®°å½•æŸ¥è¯¢

---

## ğŸ” 8.4 RAG (Retrieval-Augmented Generation)

### æ¦‚å¿µå®šä¹‰
**RAG**: æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œé€šè¿‡æ£€ç´¢ç›¸å…³æ–‡æ¡£æ¥å¢å¼ºLLMçš„ç”Ÿæˆèƒ½åŠ›

### RAGå·¥ä½œæµç¨‹

```
ç”¨æˆ·é—®é¢˜ 
  â†“
1. æ–‡æ¡£åˆ†å— (Chunking)
  â†“
2. å‘é‡åŒ– (Embedding)
  â†“
3. å­˜å‚¨ (Vector Store)
  â†“
4. æ£€ç´¢ (Retrieval) â† ç”¨æˆ·æŸ¥è¯¢å‘é‡åŒ–
  â†“
5. å¢å¼ºç”Ÿæˆ (Augmented Generation)
  â†“
æœ€ç»ˆç­”æ¡ˆ
```

### è¯¦ç»†æ­¥éª¤

#### Step 1: æ–‡æ¡£åˆ†å— (Chunking)

**ä¸ºä»€ä¹ˆéœ€è¦åˆ†å—ï¼Ÿ**
- æ–‡æ¡£å¤ªé•¿ï¼Œæ— æ³•æ•´ä½“å¤„ç†
- éœ€è¦ç²¾ç¡®å®šä½ç›¸å…³ä¿¡æ¯
- ä¼˜åŒ–æ£€ç´¢å‡†ç¡®æ€§

**åˆ†å—ç­–ç•¥**:

```python
# ç­–ç•¥1: å›ºå®šé•¿åº¦åˆ†å—
def chunk_by_tokens(text, chunk_size=500, overlap=50):
    tokens = text.split()
    chunks = []
    for i in range(0, len(tokens), chunk_size - overlap):
        chunk = ' '.join(tokens[i:i + chunk_size])
        chunks.append(chunk)
    return chunks

# ç­–ç•¥2: å¥å­åˆ†å—
def chunk_by_sentences(text, sentences_per_chunk=5):
    sentences = text.split('ã€‚')
    chunks = []
    for i in range(0, len(sentences), sentences_per_chunk):
        chunk = 'ã€‚'.join(sentences[i:i + sentences_per_chunk])
        chunks.append(chunk)
    return chunks

# ç­–ç•¥3: æ®µè½åˆ†å—
def chunk_by_paragraphs(text):
    return text.split('\n\n')
```

**åˆ†å—å‚æ•°å»ºè®®**:
- **å—å¤§å°**: 500-1000 tokens
- **é‡å **: 50-100 tokensï¼ˆé¿å…æˆªæ–­é‡è¦ä¿¡æ¯ï¼‰
- **æƒè¡¡**: å—å¤ªå°â†’ä¸Šä¸‹æ–‡ä¸è¶³ï¼›å—å¤ªå¤§â†’æ£€ç´¢ä¸ç²¾ç¡®

#### Step 2: å‘é‡åŒ– (Embedding)

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

chunks = ["Agentçš„è®°å¿†ç³»ç»Ÿ...", "RAGæŠ€æœ¯åŸç†..."]
embeddings = model.encode(chunks)
```

#### Step 3: å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“

```python
import chromadb

client = chromadb.Client()
collection = client.create_collection("knowledge_base")

collection.add(
    documents=chunks,
    embeddings=embeddings.tolist(),
    ids=[f"chunk_{i}" for i in range(len(chunks))]
)
```

#### Step 4: æ£€ç´¢ç›¸å…³æ–‡æ¡£

```python
# ç”¨æˆ·æé—®
query = "ä»€ä¹ˆæ˜¯Agentçš„è®°å¿†ç³»ç»Ÿï¼Ÿ"

# æ£€ç´¢Top-Kç›¸å…³æ–‡æ¡£
results = collection.query(
    query_texts=[query],
    n_results=5  # Top-5
)

relevant_docs = results['documents'][0]
```

#### Step 5: å¢å¼ºç”Ÿæˆ

```python
# æ„å»ºå¢å¼ºçš„Prompt
context = "\n\n".join(relevant_docs)
prompt = f"""
åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡:
{context}

é—®é¢˜: {query}

å›ç­”:
"""

# è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ
response = llm(prompt)
```

### RAGä¼˜åŒ–æŠ€æœ¯

#### 1. **æ£€ç´¢ç­–ç•¥ä¼˜åŒ–**

**Top-Kæ£€ç´¢**:
```python
# ç®€å•çš„ç›¸ä¼¼åº¦æ’åº
results = collection.query(query_texts=[query], n_results=5)
```

**MMR (Maximal Marginal Relevance)**:
```python
# å¹³è¡¡ç›¸å…³æ€§å’Œå¤šæ ·æ€§
# é¿å…æ£€ç´¢åˆ°é‡å¤å†…å®¹
results = collection.query(
    query_texts=[query],
    n_results=5,
    # æŸäº›å‘é‡æ•°æ®åº“æ”¯æŒMMR
)
```

#### 2. **Reranking (é‡æ’åº)**

```python
from sentence_transformers import CrossEncoder

# ä½¿ç”¨äº¤å‰ç¼–ç å™¨é‡æ’åº
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

# å¯¹åˆæ­¥æ£€ç´¢ç»“æœé‡æ’åº
scores = reranker.predict([(query, doc) for doc in relevant_docs])
reranked_docs = [doc for _, doc in sorted(zip(scores, relevant_docs), reverse=True)]
```

#### 3. **æ··åˆæ£€ç´¢ (Hybrid Search)**

```python
# ç»“åˆå…³é”®è¯æ£€ç´¢ï¼ˆBM25ï¼‰å’Œè¯­ä¹‰æ£€ç´¢
# 1. BM25æ£€ç´¢
keyword_results = bm25_search(query, top_k=10)

# 2. å‘é‡æ£€ç´¢
semantic_results = vector_search(query, top_k=10)

# 3. èåˆç»“æœ
final_results = reciprocal_rank_fusion(keyword_results, semantic_results)
```

### RAG vs Fine-tuning

| å¯¹æ¯”ç»´åº¦ | RAG | Fine-tuning |
|----------|-----|-------------|
| **æˆæœ¬** | ä½ï¼ˆæ— éœ€è®­ç»ƒï¼‰ | é«˜ï¼ˆéœ€è¦GPUè®­ç»ƒï¼‰ |
| **æ›´æ–°** | å®æ—¶ï¼ˆæ·»åŠ æ–‡æ¡£å³å¯ï¼‰ | å›°éš¾ï¼ˆéœ€è¦é‡æ–°è®­ç»ƒï¼‰ |
| **å¯è§£é‡Šæ€§** | é«˜ï¼ˆå¯æŸ¥çœ‹æ£€ç´¢æ–‡æ¡£ï¼‰ | ä½ï¼ˆé»‘ç›’æ¨¡å‹ï¼‰ |
| **å‡†ç¡®æ€§** | ä¾èµ–æ£€ç´¢è´¨é‡ | è¾ƒé«˜ |
| **é€‚ç”¨åœºæ™¯** | çŸ¥è¯†åº“é—®ç­”ã€æ–‡æ¡£æ£€ç´¢ | ç‰¹å®šä»»åŠ¡ä¼˜åŒ– |

---

## ğŸ§  8.5 è®°å¿†ç®¡ç†ç­–ç•¥

### 1. è®°å¿†é‡è¦æ€§è¯„åˆ†

```python
def calculate_importance(memory):
    """
    è¯„ä¼°è®°å¿†é‡è¦æ€§
    """
    score = 0.0
    
    # å› ç´ 1: æ—¶é—´æ–°é²œåº¦ï¼ˆè¶Šæ–°è¶Šé‡è¦ï¼‰
    time_score = 1.0 / (1 + time_since_creation(memory))
    
    # å› ç´ 2: è®¿é—®é¢‘ç‡ï¼ˆè¶Šå¸¸è®¿é—®è¶Šé‡è¦ï¼‰
    access_score = memory.access_count * 0.1
    
    # å› ç´ 3: è¯­ä¹‰é‡è¦æ€§ï¼ˆå…³é”®ä¿¡æ¯ï¼‰
    semantic_score = llm_evaluate_importance(memory.content)
    
    score = 0.3 * time_score + 0.3 * access_score + 0.4 * semantic_score
    return score
```

### 2. è®°å¿†é—å¿˜æœºåˆ¶

#### ç­–ç•¥1: åŸºäºæ—¶é—´è¡°å‡
```python
import math
from datetime import datetime, timedelta

def should_forget(memory, decay_rate=0.1):
    """
    åŸºäºæ—¶é—´è¡°å‡å†³å®šæ˜¯å¦é—å¿˜
    """
    days_old = (datetime.now() - memory.created_at).days
    retention = math.exp(-decay_rate * days_old)
    
    # ä¿ç•™æ¦‚ç‡ä½äºé˜ˆå€¼æ—¶åˆ é™¤
    return retention < 0.1
```

#### ç­–ç•¥2: åŸºäºè®¿é—®é¢‘ç‡
```python
def prune_memories(memories, keep_ratio=0.7):
    """
    ä¿ç•™æœ€å¸¸è®¿é—®çš„è®°å¿†
    """
    memories.sort(key=lambda m: m.access_count, reverse=True)
    keep_count = int(len(memories) * keep_ratio)
    return memories[:keep_count]
```

### 3. è®°å¿†æ•´åˆä¸æ€»ç»“

```python
def consolidate_memories(memories):
    """
    å°†ç›¸ä¼¼çš„è®°å¿†æ•´åˆæˆæ‘˜è¦
    """
    # 1. èšç±»ç›¸ä¼¼è®°å¿†
    clusters = cluster_similar_memories(memories)
    
    # 2. ä¸ºæ¯ä¸ªèšç±»ç”Ÿæˆæ‘˜è¦
    consolidated = []
    for cluster in clusters:
        summary = llm_summarize(cluster)
        consolidated.append(summary)
    
    return consolidated
```

### 4. è®°å¿†ç´¢å¼•ä¼˜åŒ–

```python
class MemoryIndex:
    def __init__(self):
        self.time_index = {}      # æ—¶é—´ç´¢å¼•
        self.topic_index = {}     # ä¸»é¢˜ç´¢å¼•
        self.importance_index = {}  # é‡è¦æ€§ç´¢å¼•
    
    def add_memory(self, memory):
        # å¤šç»´åº¦ç´¢å¼•
        self.time_index[memory.timestamp] = memory
        self.topic_index[memory.topic].append(memory)
        self.importance_index[memory.importance].append(memory)
    
    def search(self, criteria):
        # å¿«é€Ÿå¤šç»´åº¦æ£€ç´¢
        pass
```

---

## ğŸ’» 8.6 å®æˆ˜ï¼šæ„å»ºå¸¦è®°å¿†çš„Agent

### MemoryAgentæ¶æ„

```python
class MemoryAgent:
    def __init__(self, llm, short_term_memory, long_term_memory):
        self.llm = llm
        self.short_term = short_term_memory  # å¯¹è¯å†å²
        self.long_term = long_term_memory    # çŸ¥è¯†åº“
    
    def run(self, user_input):
        # 1. ä»é•¿æœŸè®°å¿†æ£€ç´¢ç›¸å…³ä¿¡æ¯
        relevant_context = self.long_term.search(user_input, top_k=3)
        
        # 2. è·å–çŸ­æœŸè®°å¿†ï¼ˆå¯¹è¯å†å²ï¼‰
        conversation_history = self.short_term.get_recent_messages(limit=5)
        
        # 3. æ„å»ºå¢å¼ºçš„Prompt
        prompt = self.build_prompt(user_input, relevant_context, conversation_history)
        
        # 4. LLMç”Ÿæˆå›å¤
        response = self.llm(prompt)
        
        # 5. æ›´æ–°è®°å¿†
        self.short_term.add_message("user", user_input)
        self.short_term.add_message("assistant", response)
        
        # 6. å­˜å‚¨é‡è¦ä¿¡æ¯åˆ°é•¿æœŸè®°å¿†
        if self.is_important(user_input):
            self.long_term.store(user_input)
        
        return response
    
    def build_prompt(self, query, context, history):
        return f"""
        å¯¹è¯å†å²:
        {history}
        
        ç›¸å…³çŸ¥è¯†:
        {context}
        
        ç”¨æˆ·: {query}
        åŠ©æ‰‹:
        """
```

---

## ğŸ¯ æ ¸å¿ƒè¦ç‚¹æ€»ç»“

### çŸ­æœŸè®°å¿†
- âœ… ç»´æŠ¤å¯¹è¯ä¸Šä¸‹æ–‡
- âœ… æ»‘åŠ¨çª—å£ç®¡ç†
- âœ… Tokenä¼˜åŒ–
- âœ… ä¸´æ—¶å­˜å‚¨

### é•¿æœŸè®°å¿†
- âœ… å‘é‡æ•°æ®åº“å­˜å‚¨
- âœ… è¯­ä¹‰æ£€ç´¢
- âœ… æŒä¹…åŒ–çŸ¥è¯†
- âœ… çŸ¥è¯†ç§¯ç´¯

### RAGæŠ€æœ¯
- âœ… æ–‡æ¡£åˆ†å— â†’ å‘é‡åŒ– â†’ å­˜å‚¨ â†’ æ£€ç´¢ â†’ å¢å¼ºç”Ÿæˆ
- âœ… ä¼˜åŒ–ï¼šRerankingã€æ··åˆæ£€ç´¢ã€MMR
- âœ… é€‚ç”¨äºçŸ¥è¯†åº“é—®ç­”

### è®°å¿†ç®¡ç†
- âœ… é‡è¦æ€§è¯„åˆ†
- âœ… é—å¿˜æœºåˆ¶
- âœ… è®°å¿†æ•´åˆ
- âœ… ç´¢å¼•ä¼˜åŒ–

---

## ğŸ“š ä¸‹ä¸€æ­¥å­¦ä¹ 

1. âœ… å®Œæˆä»£ç å®è·µï¼šå®ç°å„ä¸ªæ¨¡å—
2. âœ… å®Œæˆä¹ é¢˜ï¼šå·©å›ºç†è§£
3. âœ… é˜…è¯»æ‰©å±•èµ„æ–™ï¼šRAGè®ºæ–‡ã€å‘é‡æ•°æ®åº“æ–‡æ¡£
4. âœ… å®éªŒå¯¹æ¯”ï¼šä¸åŒåˆ†å—ç­–ç•¥ã€ä¸åŒEmbeddingæ¨¡å‹

---

**å­¦ä¹ ç¬”è®°åˆ›å»ºæ—¶é—´**: 2025-12-22  
**ä¸‹æ¬¡æ›´æ–°**: å®Œæˆä»£ç å®è·µåè¡¥å……å®æˆ˜ç»éªŒ
