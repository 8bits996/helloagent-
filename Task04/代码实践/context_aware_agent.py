"""
ä¸Šä¸‹æ–‡æ„ŸçŸ¥ Agent (Context-Aware Agent)

æ•´åˆä¸Šä¸‹æ–‡ç®¡ç†å’Œä¼˜åŒ–èƒ½åŠ›çš„æ™ºèƒ½Agent:
1. è‡ªåŠ¨ç®¡ç†å¯¹è¯å†å²
2. æ™ºèƒ½ä¼˜åŒ–ä¸Šä¸‹æ–‡
3. æˆæœ¬ç›‘æ§å’Œæ§åˆ¶
4. é€‚åº”ä¸åŒä»»åŠ¡åœºæ™¯

Author: Franke Chen
Date: 2024-12-22
"""

from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import sys
import os

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from context_manager import HybridContextManager, Message, estimate_tokens
from context_optimizer import ContextOptimizer, OptimizationResult


@dataclass
class AgentConfig:
    """Agenté…ç½®"""
    max_tokens: int = 4000  # æœ€å¤§Tokené™åˆ¶
    keep_recent: int = 3    # å¼ºåˆ¶ä¿ç•™æœ€è¿‘Næ¡
    decay_factor: float = 0.95  # æ—¶é—´è¡°å‡å› å­
    optimization_strategy: str = "auto"  # ä¼˜åŒ–ç­–ç•¥
    enable_summarization: bool = False  # æ˜¯å¦å¯ç”¨æ€»ç»“
    cost_tracking: bool = True  # æ˜¯å¦è¿½è¸ªæˆæœ¬


@dataclass
class AgentStats:
    """Agentç»Ÿè®¡ä¿¡æ¯"""
    total_queries: int = 0
    total_tokens_used: int = 0
    total_messages: int = 0
    context_compressions: int = 0
    estimated_cost: float = 0.0
    
    def __str__(self) -> str:
        return f"""
Agentç»Ÿè®¡:
  æ€»æŸ¥è¯¢æ•°: {self.total_queries}
  æ€»Tokenæ¶ˆè€—: {self.total_tokens_used}
  æ€»æ¶ˆæ¯æ•°: {self.total_messages}
  ä¸Šä¸‹æ–‡å‹ç¼©æ¬¡æ•°: {self.context_compressions}
  ä¼°ç®—æˆæœ¬: ${self.estimated_cost:.4f}
"""


class ContextAwareAgent:
    """
    ä¸Šä¸‹æ–‡æ„ŸçŸ¥Agent
    
    ç‰¹ç‚¹:
    - è‡ªåŠ¨ç®¡ç†å¯¹è¯å†å²
    - æ™ºèƒ½ä¼˜åŒ–ä¸Šä¸‹æ–‡
    - æˆæœ¬ç›‘æ§
    - çµæ´»é…ç½®
    """
    
    def __init__(
        self,
        llm_client=None,
        config: Optional[AgentConfig] = None
    ):
        """
        åˆå§‹åŒ–
        
        Args:
            llm_client: LLMå®¢æˆ·ç«¯
            config: Agenté…ç½®
        """
        self.llm_client = llm_client
        self.config = config or AgentConfig()
        
        # ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        self.context_manager = HybridContextManager(
            max_tokens=self.config.max_tokens,
            keep_recent=self.config.keep_recent,
            decay_factor=self.config.decay_factor
        )
        
        # ä¸Šä¸‹æ–‡ä¼˜åŒ–å™¨
        self.optimizer = ContextOptimizer(llm_client)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = AgentStats()
        
        # ç³»ç»Ÿæç¤ºè¯
        self.system_prompt = "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚"
    
    def set_system_prompt(self, prompt: str) -> None:
        """è®¾ç½®ç³»ç»Ÿæç¤ºè¯"""
        self.system_prompt = prompt
        self.context_manager.add_message(
            role="system",
            content=prompt,
            importance=10.0
        )
    
    def chat(self, user_message: str, importance: float = 5.0) -> str:
        """
        å¤„ç†ç”¨æˆ·æ¶ˆæ¯
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            importance: æ¶ˆæ¯é‡è¦æ€§ (0-10)
            
        Returns:
            Agentå›å¤
        """
        # 1. æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        self.context_manager.add_message(
            role="user",
            content=user_message,
            importance=importance
        )
        
        # 2. è·å–å¹¶ä¼˜åŒ–ä¸Šä¸‹æ–‡
        context = self._prepare_context(user_message)
        
        # 3. è°ƒç”¨LLM
        if self.llm_client:
            try:
                assistant_message = self.llm_client.chat(context)
            except Exception as e:
                assistant_message = f"æŠ±æ­‰,å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}"
        else:
            assistant_message = "[æ¨¡æ‹Ÿå›å¤] è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„Agentå›å¤ã€‚"
        
        # 4. æ·»åŠ åŠ©æ‰‹å›å¤
        self.context_manager.add_message(
            role="assistant",
            content=assistant_message,
            importance=importance
        )
        
        # 5. æ›´æ–°ç»Ÿè®¡
        self._update_stats(context)
        
        return assistant_message
    
    def _prepare_context(self, current_query: str) -> List[Dict[str, str]]:
        """
        å‡†å¤‡ä¸Šä¸‹æ–‡
        
        æ ¹æ®é…ç½®é€‰æ‹©ä¸åŒçš„ç­–ç•¥:
        - åŸºç¡€ç­–ç•¥: ç›´æ¥ä½¿ç”¨HybridContextManager
        - ä¼˜åŒ–ç­–ç•¥: è¿›ä¸€æ­¥ä¼˜åŒ–ä¸Šä¸‹æ–‡
        
        Args:
            current_query: å½“å‰æŸ¥è¯¢
            
        Returns:
            å‡†å¤‡å¥½çš„ä¸Šä¸‹æ–‡
        """
        # è·å–åŸºç¡€ä¸Šä¸‹æ–‡
        base_context = self.context_manager.get_context()
        
        # è½¬æ¢ä¸ºMessageå¯¹è±¡(ç”¨äºä¼˜åŒ–)
        messages = [
            Message(
                role=msg["role"],
                content=msg["content"],
                importance=5.0
            )
            for msg in base_context
        ]
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¼˜åŒ–
        total_tokens = sum(estimate_tokens(msg["content"]) for msg in base_context)
        
        if total_tokens > self.config.max_tokens * 0.8:
            # å¦‚æœæ¥è¿‘Tokené™åˆ¶,è¿›è¡Œä¼˜åŒ–
            optimized, result = self.optimizer.optimize(
                messages,
                target_tokens=int(self.config.max_tokens * 0.7),
                strategy=self.config.optimization_strategy
            )
            
            self.stats.context_compressions += 1
            
            return [msg.to_dict() for msg in optimized]
        
        return base_context
    
    def _update_stats(self, context: List[Dict[str, str]]) -> None:
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        if not self.config.cost_tracking:
            return
        
        self.stats.total_queries += 1
        self.stats.total_messages = self.context_manager.count_messages()
        
        # è®¡ç®—Tokenæ¶ˆè€—
        tokens = sum(estimate_tokens(msg["content"]) for msg in context)
        self.stats.total_tokens_used += tokens
        
        # ä¼°ç®—æˆæœ¬ (å‡è®¾GPT-4å®šä»·)
        input_cost = tokens * 0.03 / 1000
        output_cost = (tokens * 0.5) * 0.06 / 1000  # å‡è®¾è¾“å‡ºæ˜¯è¾“å…¥çš„50%
        self.stats.estimated_cost += (input_cost + output_cost)
    
    def get_stats(self) -> AgentStats:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats
    
    def get_context_summary(self) -> Dict[str, Any]:
        """è·å–ä¸Šä¸‹æ–‡æ‘˜è¦"""
        context = self.context_manager.get_context()
        total_tokens = sum(estimate_tokens(msg["content"]) for msg in context)
        
        return {
            "total_messages": self.context_manager.count_messages(),
            "context_messages": len(context),
            "total_tokens": total_tokens,
            "max_tokens": self.config.max_tokens,
            "token_usage_ratio": total_tokens / self.config.max_tokens,
            "system_messages": sum(1 for msg in context if msg["role"] == "system"),
            "user_messages": sum(1 for msg in context if msg["role"] == "user"),
            "assistant_messages": sum(1 for msg in context if msg["role"] == "assistant")
        }
    
    def clear_history(self) -> None:
        """æ¸…ç©ºå†å²"""
        self.context_manager.clear()
        # é‡æ–°æ·»åŠ ç³»ç»Ÿæç¤ºè¯
        if self.system_prompt:
            self.context_manager.add_message(
                role="system",
                content=self.system_prompt,
                importance=10.0
            )


class MultiTaskAgent(ContextAwareAgent):
    """
    å¤šä»»åŠ¡Agent
    
    æ‰©å±•ContextAwareAgent,æ”¯æŒä¸åŒä»»åŠ¡åœºæ™¯çš„ä¸Šä¸‹æ–‡ç­–ç•¥
    """
    
    def __init__(self, llm_client=None):
        super().__init__(llm_client)
        
        # ä»»åŠ¡ç‰¹å®šé…ç½®
        self.task_configs = {
            "short_qa": AgentConfig(
                max_tokens=1000,
                keep_recent=2,
                optimization_strategy="truncate"
            ),
            "long_conversation": AgentConfig(
                max_tokens=4000,
                keep_recent=5,
                optimization_strategy="hybrid",
                enable_summarization=True
            ),
            "code_assistant": AgentConfig(
                max_tokens=8000,
                keep_recent=3,
                optimization_strategy="truncate"
            )
        }
        
        self.current_task = "long_conversation"
    
    def set_task_mode(self, task: str) -> None:
        """
        è®¾ç½®ä»»åŠ¡æ¨¡å¼
        
        Args:
            task: ä»»åŠ¡ç±»å‹ (short_qa/long_conversation/code_assistant)
        """
        if task in self.task_configs:
            self.current_task = task
            self.config = self.task_configs[task]
            
            # é‡æ–°åˆ›å»ºä¸Šä¸‹æ–‡ç®¡ç†å™¨
            self.context_manager = HybridContextManager(
                max_tokens=self.config.max_tokens,
                keep_recent=self.config.keep_recent,
                decay_factor=self.config.decay_factor
            )
            
            print(f"âœ… åˆ‡æ¢åˆ°ä»»åŠ¡æ¨¡å¼: {task}")
        else:
            print(f"âŒ æœªçŸ¥ä»»åŠ¡ç±»å‹: {task}")


# ============= æµ‹è¯•ä»£ç  =============

def test_basic_agent():
    """æµ‹è¯•åŸºç¡€AgentåŠŸèƒ½"""
    print("=" * 60)
    print("æµ‹è¯•1: åŸºç¡€Agentå¯¹è¯")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡æ‹ŸLLM
    class MockLLM:
        def chat(self, messages):
            last_user_msg = [m for m in messages if m["role"] == "user"][-1]
            return f"[æ¨¡æ‹Ÿå›å¤] æ”¶åˆ°ä½ çš„æ¶ˆæ¯: {last_user_msg['content'][:30]}..."
    
    agent = ContextAwareAgent(llm_client=MockLLM())
    agent.set_system_prompt("ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚")
    
    # å¤šè½®å¯¹è¯
    queries = [
        "ä½ å¥½!",
        "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·?",
        "æ¨èä¸€äº›å¥½ç©çš„åœ°æ–¹",
        "å¸®æˆ‘æ€»ç»“ä¸€ä¸‹æˆ‘ä»¬çš„å¯¹è¯"
    ]
    
    print("\nå¼€å§‹å¯¹è¯:\n")
    for i, query in enumerate(queries, 1):
        print(f"ç”¨æˆ· {i}: {query}")
        response = agent.chat(query)
        print(f"åŠ©æ‰‹ {i}: {response}\n")
    
    # æ˜¾ç¤ºç»Ÿè®¡
    print(agent.get_stats())
    
    # æ˜¾ç¤ºä¸Šä¸‹æ–‡æ‘˜è¦
    summary = agent.get_context_summary()
    print("ä¸Šä¸‹æ–‡æ‘˜è¦:")
    for key, value in summary.items():
        print(f"  {key}: {value}")
    
    print("\nâœ… åŸºç¡€Agentæµ‹è¯•é€šè¿‡!")


def test_context_optimization():
    """æµ‹è¯•ä¸Šä¸‹æ–‡ä¼˜åŒ–"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•2: ä¸Šä¸‹æ–‡è‡ªåŠ¨ä¼˜åŒ–")
    print("=" * 60)
    
    class MockLLM:
        def chat(self, messages):
            return "è¿™æ˜¯ä¸€ä¸ªå›å¤ã€‚"
    
    # åˆ›å»ºå°Tokené™åˆ¶çš„Agent
    config = AgentConfig(
        max_tokens=150,
        keep_recent=2,
        optimization_strategy="truncate"
    )
    
    agent = ContextAwareAgent(llm_client=MockLLM(), config=config)
    agent.set_system_prompt("ä½ æ˜¯AIåŠ©æ‰‹ã€‚")
    
    # æ·»åŠ å¤šæ¡æ¶ˆæ¯è§¦å‘ä¼˜åŒ–
    for i in range(10):
        query = f"è¿™æ˜¯ç¬¬{i+1}ä¸ªé—®é¢˜,å†…å®¹åŒ…å«å¾ˆå¤šæ–‡å­—æ¥æµ‹è¯•Tokené™åˆ¶åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚"
        agent.chat(query)
    
    summary = agent.get_context_summary()
    print(f"\næ€»æ¶ˆæ¯æ•°: {summary['total_messages']}")
    print(f"ä¸Šä¸‹æ–‡æ¶ˆæ¯æ•°: {summary['context_messages']}")
    print(f"Tokenä½¿ç”¨: {summary['total_tokens']}/{summary['max_tokens']}")
    print(f"ä½¿ç”¨ç‡: {summary['token_usage_ratio']:.1%}")
    
    assert summary['total_tokens'] <= config.max_tokens, "åº”è¯¥åœ¨Tokené™åˆ¶å†…"
    
    print("\nâœ… ä¸Šä¸‹æ–‡ä¼˜åŒ–æµ‹è¯•é€šè¿‡!")


def test_multi_task_agent():
    """æµ‹è¯•å¤šä»»åŠ¡Agent"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•3: å¤šä»»åŠ¡Agent")
    print("=" * 60)
    
    class MockLLM:
        def chat(self, messages):
            return "å›å¤å†…å®¹"
    
    agent = MultiTaskAgent(llm_client=MockLLM())
    
    # æµ‹è¯•ä¸åŒä»»åŠ¡æ¨¡å¼
    tasks = ["short_qa", "long_conversation", "code_assistant"]
    
    for task in tasks:
        print(f"\næµ‹è¯•ä»»åŠ¡æ¨¡å¼: {task}")
        agent.set_task_mode(task)
        
        # å‘é€å‡ æ¡æ¶ˆæ¯
        for i in range(3):
            agent.chat(f"æµ‹è¯•æ¶ˆæ¯ {i+1}")
        
        summary = agent.get_context_summary()
        print(f"  Tokené™åˆ¶: {summary['max_tokens']}")
        print(f"  å½“å‰ä½¿ç”¨: {summary['total_tokens']}")
        
        agent.clear_history()
    
    print("\nâœ… å¤šä»»åŠ¡Agentæµ‹è¯•é€šè¿‡!")


def test_importance_handling():
    """æµ‹è¯•é‡è¦æ€§å¤„ç†"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•4: æ¶ˆæ¯é‡è¦æ€§å¤„ç†")
    print("=" * 60)
    
    class MockLLM:
        def chat(self, messages):
            return "å›å¤"
    
    config = AgentConfig(max_tokens=100, keep_recent=2)
    agent = ContextAwareAgent(llm_client=MockLLM(), config=config)
    
    # æ·»åŠ ä¸åŒé‡è¦æ€§çš„æ¶ˆæ¯
    agent.chat("æ™®é€šæ¶ˆæ¯1", importance=3.0)
    agent.chat("æ™®é€šæ¶ˆæ¯2", importance=3.0)
    agent.chat("é‡è¦æ¶ˆæ¯!", importance=9.0)
    agent.chat("æ™®é€šæ¶ˆæ¯3", importance=3.0)
    agent.chat("æ™®é€šæ¶ˆæ¯4", importance=3.0)
    
    summary = agent.get_context_summary()
    context = agent.context_manager.get_context()
    
    print(f"\næ€»æ¶ˆæ¯æ•°: {summary['total_messages']}")
    print(f"ä¸Šä¸‹æ–‡ä¸­çš„æ¶ˆæ¯:")
    for msg in context:
        print(f"  [{msg['role']}]: {msg['content'][:40]}...")
    
    # éªŒè¯é‡è¦æ¶ˆæ¯è¢«ä¿ç•™
    contents = [msg['content'] for msg in context]
    # ç”±äºkeep_recent=2,æœ€è¿‘2æ¡ä¸€å®šä¿ç•™
    assert contents[-1] == "å›å¤", "æœ€è¿‘çš„æ¶ˆæ¯åº”è¯¥è¢«ä¿ç•™"
    
    print("\nâœ… é‡è¦æ€§å¤„ç†æµ‹è¯•é€šè¿‡!")


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\nğŸš€ å¼€å§‹æµ‹è¯•ä¸Šä¸‹æ–‡æ„ŸçŸ¥Agent...\n")
    
    test_basic_agent()
    test_context_optimization()
    test_multi_task_agent()
    test_importance_handling()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    print("=" * 60)
    
    print("\nğŸ’¡ ä¸Šä¸‹æ–‡æ„ŸçŸ¥Agentçš„ä¼˜åŠ¿:")
    print("  1. âœ… è‡ªåŠ¨ç®¡ç†å¯¹è¯å†å²,æ— éœ€æ‰‹åŠ¨ç»´æŠ¤")
    print("  2. âœ… æ™ºèƒ½ä¼˜åŒ–ä¸Šä¸‹æ–‡,æ§åˆ¶Tokenæ¶ˆè€—")
    print("  3. âœ… æˆæœ¬è¿½è¸ª,å®æ—¶ç›‘æ§APIè´¹ç”¨")
    print("  4. âœ… çµæ´»é…ç½®,é€‚åº”ä¸åŒä»»åŠ¡åœºæ™¯")
    print("  5. âœ… å¤šä»»åŠ¡æ”¯æŒ,å¯åˆ‡æ¢ä¸åŒæ¨¡å¼")


if __name__ == "__main__":
    run_all_tests()
