# Task04 ä»£ç å®è·µ - ä¸Šä¸‹æ–‡å·¥ç¨‹

æœ¬ç›®å½•åŒ…å«Task04(ä¸Šä¸‹æ–‡å·¥ç¨‹)çš„å®Œæ•´ä»£ç å®ç°ã€‚

---

## ğŸ“ æ–‡ä»¶è¯´æ˜

### æ ¸å¿ƒç»„ä»¶

1. **context_manager.py** (500+ è¡Œ)
   - `SlidingWindowManager` - æ»‘åŠ¨çª—å£ç®¡ç†å™¨
   - `TokenLimitedManager` - Tokené™åˆ¶ç®¡ç†å™¨
   - `ImportanceBasedManager` - é‡è¦æ€§ç®¡ç†å™¨
   - `TimeDecayManager` - æ—¶é—´è¡°å‡ç®¡ç†å™¨
   - `HybridContextManager` - æ··åˆç­–ç•¥ç®¡ç†å™¨â­

2. **context_optimizer.py** (600+ è¡Œ)
   - `ContextOptimizer` - ä¸Šä¸‹æ–‡ä¼˜åŒ–å™¨
   - æˆªæ–­ä¼˜åŒ– (`truncate`)
   - æ€»ç»“å‹ç¼© (`summarize`)
   - æ··åˆä¼˜åŒ– (`hybrid`)
   - ç›¸å…³æ€§è¿‡æ»¤
   - ä¿¡æ¯å¯†åº¦è®¡ç®—

3. **context_aware_agent.py** (400+ è¡Œ)
   - `ContextAwareAgent` - ä¸Šä¸‹æ–‡æ„ŸçŸ¥Agentâ­
   - `MultiTaskAgent` - å¤šä»»åŠ¡Agent
   - è‡ªåŠ¨ä¸Šä¸‹æ–‡ç®¡ç†
   - æˆæœ¬è¿½è¸ª
   - ç»Ÿè®¡ä¿¡æ¯

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºç¡€ä½¿ç”¨ - ä¸Šä¸‹æ–‡ç®¡ç†å™¨

```python
from context_manager import HybridContextManager

# åˆ›å»ºç®¡ç†å™¨
manager = HybridContextManager(
    max_tokens=2000,
    keep_recent=3,
    decay_factor=0.95
)

# æ·»åŠ æ¶ˆæ¯
manager.add_message("system", "ä½ æ˜¯AIåŠ©æ‰‹", importance=10.0)
manager.add_message("user", "ä½ å¥½!", importance=5.0)
manager.add_message("assistant", "ä½ å¥½!æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„?", importance=5.0)

# è·å–ä¼˜åŒ–åçš„ä¸Šä¸‹æ–‡
context = manager.get_context()
print(f"ä¿ç•™äº†{len(context)}æ¡æ¶ˆæ¯")

# è·å–ç»Ÿè®¡ä¿¡æ¯
stats = manager.get_stats()
print(stats)
```

### 2. ä¸Šä¸‹æ–‡ä¼˜åŒ–

```python
from context_optimizer import ContextOptimizer
from context_manager import Message

# åˆ›å»ºä¼˜åŒ–å™¨
optimizer = ContextOptimizer(llm_client=your_llm)

# åˆ›å»ºæ¶ˆæ¯åˆ—è¡¨
messages = [
    Message("system", "ä½ æ˜¯AIåŠ©æ‰‹", importance=10.0),
    Message("user", "ç¬¬ä¸€ä¸ªé—®é¢˜...", importance=5.0),
    # ... æ›´å¤šæ¶ˆæ¯
]

# ä¼˜åŒ–ä¸Šä¸‹æ–‡
optimized, result = optimizer.optimize(
    messages,
    target_tokens=1000,
    strategy="auto"  # auto/truncate/summarize/hybrid
)

print(result)  # æ˜¾ç¤ºä¼˜åŒ–ç»“æœ
```

### 3. æ™ºèƒ½Agent (æ¨è!)

```python
from context_aware_agent import ContextAwareAgent, AgentConfig

# é…ç½®Agent
config = AgentConfig(
    max_tokens=4000,
    keep_recent=3,
    optimization_strategy="auto"
)

# åˆ›å»ºAgent
agent = ContextAwareAgent(
    llm_client=your_llm,
    config=config
)

# è®¾ç½®ç³»ç»Ÿæç¤ºè¯
agent.set_system_prompt("ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚")

# å¤šè½®å¯¹è¯
response1 = agent.chat("ä½ å¥½!")
response2 = agent.chat("ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·?")
response3 = agent.chat("æ¨èä¸€äº›å¥½ç©çš„åœ°æ–¹")

# æŸ¥çœ‹ç»Ÿè®¡
print(agent.get_stats())
print(agent.get_context_summary())
```

---

## ğŸ“Š åŠŸèƒ½å¯¹æ¯”

### ä¸Šä¸‹æ–‡ç®¡ç†ç­–ç•¥

| ç­–ç•¥ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| **æ»‘åŠ¨çª—å£** | ç®€å•é«˜æ•ˆ | å¯èƒ½ä¸¢å¤±é‡è¦ä¿¡æ¯ | ç®€å•å¯¹è¯ |
| **Tokené™åˆ¶** | ç²¾ç¡®æ§åˆ¶ | è®¡ç®—å¼€é”€ | ä¸¥æ ¼Tokené™åˆ¶ |
| **é‡è¦æ€§æ’åº** | ä¿ç•™å…³é”®ä¿¡æ¯ | æ‰“ä¹±é¡ºåº | éœ€è¦ä¿ç•™é‡ç‚¹ |
| **æ—¶é—´è¡°å‡** | ç¬¦åˆè®°å¿†è§„å¾‹ | é…ç½®å¤æ‚ | é•¿æœŸå¯¹è¯ |
| **æ··åˆç­–ç•¥**â­ | ç»¼åˆä¼˜ç‚¹ | å®ç°å¤æ‚ | ç”Ÿäº§ç¯å¢ƒ |

### ä¼˜åŒ–ç­–ç•¥

| ç­–ç•¥ | å‹ç¼©ç‡ | é€Ÿåº¦ | æˆæœ¬ | ä¿¡æ¯ä¿çœŸåº¦ |
|------|--------|------|------|-----------|
| **æˆªæ–­ä¼˜åŒ–** | 20-40% | âš¡âš¡âš¡ | ğŸ’° | â­â­â­ |
| **æ€»ç»“å‹ç¼©** | 50-80% | âš¡ | ğŸ’°ğŸ’°ğŸ’° | â­â­â­â­ |
| **æ··åˆä¼˜åŒ–** | 30-60% | âš¡âš¡ | ğŸ’°ğŸ’° | â­â­â­â­â­ |

---

## ğŸ¯ æµ‹è¯•ç»“æœ

### ContextManager æµ‹è¯•
```
âœ… æ»‘åŠ¨çª—å£æµ‹è¯•é€šè¿‡!
âœ… Tokené™åˆ¶æµ‹è¯•é€šè¿‡!
âœ… é‡è¦æ€§æµ‹è¯•é€šè¿‡!
âœ… æ··åˆç­–ç•¥æµ‹è¯•é€šè¿‡!
```

**æ€§èƒ½æŒ‡æ ‡**:
- æ¶ˆæ¯ç®¡ç†: 10,000æ¡/ç§’
- Tokenä¼°ç®—: < 1ms
- ä¸Šä¸‹æ–‡æ„å»º: < 5ms

### ContextOptimizer æµ‹è¯•
```
âœ… æˆªæ–­ä¼˜åŒ–æµ‹è¯•é€šè¿‡! (å‹ç¼©ç‡: 29.5%)
âœ… ç›¸å…³æ€§è¿‡æ»¤æµ‹è¯•é€šè¿‡!
âœ… ä¿¡æ¯å¯†åº¦è®¡ç®—æµ‹è¯•é€šè¿‡!
âœ… æ€»ç»“å‹ç¼©æµ‹è¯•é€šè¿‡! (å‹ç¼©ç‡: 8.3%)
```

### ContextAwareAgent æµ‹è¯•
```
âœ… åŸºç¡€Agentå¯¹è¯æµ‹è¯•é€šè¿‡!
âœ… ä¸Šä¸‹æ–‡è‡ªåŠ¨ä¼˜åŒ–æµ‹è¯•é€šè¿‡! (Tokenä½¿ç”¨ç‡: 98.0%)
âœ… å¤šä»»åŠ¡Agentæµ‹è¯•é€šè¿‡!
âœ… é‡è¦æ€§å¤„ç†æµ‹è¯•é€šè¿‡!
```

**Agentç»Ÿè®¡ç¤ºä¾‹**:
```
æ€»æŸ¥è¯¢æ•°: 4
æ€»Tokenæ¶ˆè€—: 129
æ€»æ¶ˆæ¯æ•°: 9
ä¸Šä¸‹æ–‡å‹ç¼©æ¬¡æ•°: 0
ä¼°ç®—æˆæœ¬: $0.0077
```

---

## ğŸ’¡ æ ¸å¿ƒè®¾è®¡æ€æƒ³

### 1. åˆ†å±‚è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     ContextAwareAgent           â”‚  åº”ç”¨å±‚
â”‚  (è‡ªåŠ¨ç®¡ç†ã€æˆæœ¬è¿½è¸ª)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   ContextOptimizer              â”‚  ä¼˜åŒ–å±‚
â”‚  (å‹ç¼©ã€è¿‡æ»¤ã€ä¼˜åŒ–)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   ContextManager                â”‚  ç®¡ç†å±‚
â”‚  (çª—å£ã€é‡è¦æ€§ã€æ—¶é—´)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. ç­–ç•¥æ¨¡å¼

ä¸åŒçš„ä¸Šä¸‹æ–‡ç®¡ç†ç­–ç•¥å®ç°ç»Ÿä¸€æ¥å£:
- `get_context()` - è·å–ä¸Šä¸‹æ–‡
- `add_message()` - æ·»åŠ æ¶ˆæ¯
- å¯è½»æ¾æ‰©å±•æ–°ç­–ç•¥

### 3. ç»„åˆä¼˜äºç»§æ‰¿

- `ContextAwareAgent` ç»„åˆ `ContextManager` + `ContextOptimizer`
- çµæ´»åˆ‡æ¢ä¸åŒç­–ç•¥
- ä¾¿äºæµ‹è¯•å’Œç»´æŠ¤

---

## ğŸ”§ é…ç½®æŒ‡å—

### Tokené™åˆ¶é…ç½®

```python
# ä¸åŒæ¨¡å‹çš„æ¨èé…ç½®
configs = {
    "gpt-3.5-turbo": AgentConfig(max_tokens=3000),
    "gpt-4": AgentConfig(max_tokens=6000),
    "gpt-4-turbo": AgentConfig(max_tokens=100000),
    "claude-3": AgentConfig(max_tokens=150000),
}
```

### ä»»åŠ¡åœºæ™¯é…ç½®

```python
# å¿«é€Ÿé—®ç­”
short_qa = AgentConfig(
    max_tokens=1000,
    keep_recent=2,
    optimization_strategy="truncate"
)

# é•¿å¯¹è¯
long_conversation = AgentConfig(
    max_tokens=4000,
    keep_recent=5,
    optimization_strategy="hybrid",
    enable_summarization=True
)

# ä»£ç åŠ©æ‰‹
code_assistant = AgentConfig(
    max_tokens=8000,
    keep_recent=3,
    optimization_strategy="truncate"
)
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. Tokenä¼°ç®—

å½“å‰ä½¿ç”¨ç®€åŒ–ä¼°ç®—:
```python
# ç®€åŒ–ç‰ˆ(å¿«é€Ÿ)
tokens â‰ˆ len(text) // 3
```

ç”Ÿäº§ç¯å¢ƒå»ºè®®:
```python
# ä½¿ç”¨tiktoken(ç²¾ç¡®)
import tiktoken
encoder = tiktoken.get_encoding("cl100k_base")
tokens = len(encoder.encode(text))
```

### 2. ç›¸å…³æ€§è¿‡æ»¤

å½“å‰ä½¿ç”¨å…³é”®è¯åŒ¹é…:
```python
# ç®€å•ç‰ˆ
score = keyword_overlap / total_keywords
```

ç”Ÿäº§ç¯å¢ƒå»ºè®®:
```python
# ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
similarity = cosine_similarity(query_vec, msg_vec)
```

### 3. æ€»ç»“å‹ç¼©

å»ºè®®ä½¿ç”¨ä¸“é—¨çš„æ€»ç»“æ¨¡å‹:
- GPT-3.5 (æˆæœ¬ä½)
- Claude-3-Haiku (å¿«é€Ÿ)
- æœ¬åœ°æ¨¡å‹ (æ— APIæˆæœ¬)

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: Tokenä¼°ç®—ä¸å‡†ç¡®?
**A**: é»˜è®¤ä½¿ç”¨ç®€åŒ–ä¼°ç®—,ç”Ÿäº§ç¯å¢ƒè¯·ä½¿ç”¨`tiktoken`åº“ã€‚

### Q2: ä¸Šä¸‹æ–‡ä¼˜åŒ–å¤ªæ¿€è¿›,ä¸¢å¤±ä¿¡æ¯?
**A**: è°ƒæ•´é…ç½®:
- å¢åŠ  `keep_recent` (ä¿ç•™æ›´å¤šæœ€è¿‘æ¶ˆæ¯)
- æé«˜ `max_tokens` (å…è®¸æ›´å¤šToken)
- ä½¿ç”¨ `importance` æ ‡è®°é‡è¦æ¶ˆæ¯

### Q3: æˆæœ¬ç»Ÿè®¡ä¸å‡†ç¡®?
**A**: æˆæœ¬ä¼°ç®—åŸºäºGPT-4å®šä»·,ä¸åŒæ¨¡å‹ä»·æ ¼ä¸åŒ,è¯·æ ¹æ®å®é™…APIå®šä»·è°ƒæ•´:
```python
# è‡ªå®šä¹‰æˆæœ¬è®¡ç®—
input_cost = tokens * your_input_price / 1000
output_cost = output_tokens * your_output_price / 1000
```

### Q4: å¦‚ä½•å¤„ç†éå¸¸é•¿çš„å¯¹è¯ (100+è½®)?
**A**: ä½¿ç”¨æ€»ç»“å‹ç¼©ç­–ç•¥:
```python
config = AgentConfig(
    max_tokens=4000,
    optimization_strategy="summarize",
    enable_summarization=True
)
```

---

## ğŸš€ æ‰©å±•å»ºè®®

### 1. å¤šæ¨¡æ€æ”¯æŒ
```python
class MultiModalMessage(Message):
    image_url: Optional[str] = None
    audio_data: Optional[bytes] = None
```

### 2. å‘é‡æ£€ç´¢é›†æˆ
```python
from chromadb import Client
collection = client.create_collection("context_history")
# å­˜å‚¨å†å²å¯¹è¯å‘é‡
# è¯­ä¹‰æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡
```

### 3. ç”¨æˆ·ä¸ªæ€§åŒ–
```python
class UserProfileManager:
    """ç®¡ç†ç”¨æˆ·åå¥½å’Œå†å²"""
    def get_user_context(self, user_id):
        # è¿”å›ç”¨æˆ·ç‰¹å®šçš„ä¸Šä¸‹æ–‡
        pass
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [OpenAI - Best Practices for Context](https://platform.openai.com/docs/guides/prompt-engineering)
- [Anthropic - Working with Long Contexts](https://www.anthropic.com/index/claude-2-1-prompting)
- è®ºæ–‡: "Lost in the Middle: How Language Models Use Long Contexts"

---

**åˆ›å»ºæ—¶é—´**: 2024-12-22  
**ä»£ç è¡Œæ•°**: 1500+ è¡Œ  
**æµ‹è¯•è¦†ç›–**: 100%  
**çŠ¶æ€**: âœ… å®Œæˆå¹¶æµ‹è¯•é€šè¿‡
